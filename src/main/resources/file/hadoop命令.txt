
1.hadoop
hadoop1(192.168.1.221)
hadoop1(192.168.1.222)
hadoop1(192.168.1.223)

启动
/usr/local/hadoop/sbin/start-all.sh
停止
/usr/local/hadoop/sbin/stop-all.sh
网页
http://192.168.1.221:50070/dfshealth.html#tab-overview
http://192.168.1.221:8088/cluster


2.hbase

zookeeper(192.168.1.224)
启动
/usr/local/zookeeper/bin/zkServer.sh start /usr/local/zookeeper/bin/zk0.cfg
/usr/local/zookeeper/bin/zkServer.sh start /usr/local/zookeeper/bin/zk1.cfg
/usr/local/zookeeper/bin/zkServer.sh start /usr/local/zookeeper/bin/zk2.cfg
查看zk状态
/usr/local/zookeeper/bin/zkServer.sh status /usr/local/zookeeper/bin/zk0.cfg
/usr/local/zookeeper/bin/zkServer.sh status /usr/local/zookeeper/bin/zk1.cfg
/usr/local/zookeeper/bin/zkServer.sh status /usr/local/zookeeper/bin/zk2.cfg

hbase三台机器
hadoop1(192.168.1.221)
hadoop1(192.168.1.222)
hadoop1(192.168.1.223)

启动
/usr/local/hbase/bin/start-hbase.sh
停止
/usr/local/hbase/bin/stop-hbase.sh
页面
http://192.168.1.221:16010/master-status

进入shell命令
/usr/local/hbase/bin/hbase shell
create 'table_name','column_family'	---建表 
list				---查看所有表


3.hive

启动mysql
service mysql start
service mysql status

启动hive
/usr/local/hive/bin/hive --service hiveserver2 &
连接hive
/usr/local/hive/bin/beeline
!connect jdbc:hive2://hadoop1:10000
连接测试
show databases

4.spark

spark的mllib工具包比如线性回归，logistic回归，贝叶斯分类，svm，决策树，随机森林

启动：
/usr/local/spark/sbin/start-all.sh
停止：
/usr/local/spark/sbin/stop-all.sh

网页:
http://192.168.1.221:8080/

计算圆周率：
/usr/local/spark/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client /usr/local/spark/examples/jars/spark-examples_2.11-2.1.3.jar


